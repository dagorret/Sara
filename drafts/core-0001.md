¿CLI con parámetros o con JSON?

La mejor opción para SARA es ambas, pero con una regla:

Comandos simples → parámetros CLI

Operaciones complejas / reproducibles → JSON (o YAML) como “spec”


Así tenés:

UX rápida (parámetros)

y reproducibilidad + automatización (specs) para pipelines, docencia y profesional.



---

1) Diseño del orquestador CLI (modelo mental)

El CLI es una capa fina que hace:

1. Parsea inputs (flags o spec)


2. Valida (dataset existe, columna existe, tipos)


3. Ejecuta (DuckDB + Parquet)


4. Registra (Postgres: Dataset/DatasetVersion/Operation/Run/Artifact)


5. Escribe outputs (Parquet y artifacts)




---

2) Propuesta de comandos (MVP real)

A) Datasets

sara dataset create --name "Hogares 2023" --id hogares_2023
sara dataset list
sara dataset show hogares_2023

B) Importación (crea v1)

sara import csv hogares_2023 data/raw/hogares.csv
sara import xlsx hogares_2023 data/raw/hogares.xlsx --sheet "Hoja1"

C) Preview / perfilado (cuellos de botella típicos)

sara preview hogares_2023:v1 --limit 100
sara profile hogares_2023:v1
sara count hogares_2023:v1 --where "edad>=18"

D) Transformaciones (Operations → crean nueva versión)

sara transform filter hogares_2023:v1 --where "edad>=18"
sara transform winsorize hogares_2023:v2 --col ingreso --p-low 0.01 --p-high 0.99 --out-col ingreso_w
sara transform recode hogares_2023:v2 --col sexo --map '{"M":1,"F":0}' --out-col sexo_bin

E) Estadística / análisis (Runs → artifacts)

sara stats mean hogares_2023:v3 --col ingreso_w
sara stats mean hogares_2023:v3 --col ingreso_w --where "provincia='Córdoba'"


---

3) ¿Dónde entra el JSON?

Para cosas complejas: por ejemplo una transformación larga o una corrida econométrica con muchas opciones.

Ejemplo

sara run from-spec specs/did_basic.json

Ese JSON debería describir:

dataset_version

método

parámetros

outputs


Es el “artefacto reproducible” que después en web se vuelve un wizard.


---

4) Cuellos de botella que conviene medir con CLI (antes de UX)

Estos son los que te conviene testear ya:

Importación

CSV grande → tiempo de lectura + conversión a Parquet

XLSX grande → lento; probablemente limitarlo o convertirlo a CSV


Preview y filtros

preview siempre con LIMIT

contar y filtrar deben hacerse con DuckDB, sin pandas


Transformaciones

operaciones que requieren percentiles (winsorize) → medir cuantiles

recodes masivos → eficiencia del CASE WHEN


Exportación

XLSX y PDF pueden volverse lentos si son enormes

conviene limitar exports a “resúmenes” y no a datasets enteros



---

5) Andamiaje para Desktop y Web (la clave)

Si hacés el CLI bien, el resto sale “gratis”.

Arquitectura recomendada

core/ (entidades + invariantes)

engine/ (DuckDB)

storage/ (Parquet paths + Postgres repo)

orchestrator/ (servicios)

cli/ (Typer/Click)

api/ (FastAPI/Django REST) → llama al mismo orchestrator


Desktop

corre el CLI directo

o una GUI simple que llama al orquestador


Web

UI → API → orquestador

sin duplicar lógica



---
